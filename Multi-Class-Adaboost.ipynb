{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "vOfE7N7mTyrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "rFfw_kt5T0wh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi Class Perceptron"
      ],
      "metadata": {
        "id": "2IkiBX7DWbMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiClassPerceptron:\n",
        "    \"\"\"\n",
        "    Implementation of a multi-class perceptron as a weak learner.\n",
        "    Supports weighted samples for integration with ensemble methods like Adaboost.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.1, n_iterations=100):\n",
        "        \"\"\"\n",
        "        Initialize the perceptron with learning rate and number of iterations.\n",
        "\n",
        "        Args:\n",
        "            alpha (float): Learning rate for weight updates.\n",
        "            n_iterations (int): Maximum number of iterations for training.\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = {}\n",
        "        self.biases = {}\n",
        "\n",
        "    def train(self, X, y, sample_weights=None, weak_threshold=0.55):\n",
        "        \"\"\"\n",
        "        Train the perceptron for multi-class classification.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Training data, shape (n_samples, n_features).\n",
        "            y (np.ndarray): Training labels, shape (n_samples,).\n",
        "            sample_weights (np.ndarray, optional): Weights for training samples.\n",
        "            weak_threshold (float): Minimum accuracy to stop training early.\n",
        "\n",
        "        Returns:\n",
        "            self: Trained MultiClassPerceptron instance.\n",
        "        \"\"\"\n",
        "        self.classes = np.unique(y)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize sample weights if not provided\n",
        "        if sample_weights is None:\n",
        "            sample_weights = np.ones(n_samples) / n_samples\n",
        "\n",
        "        for cls in self.classes:\n",
        "            # Initialize weights and biases for the current class\n",
        "            self.weights[cls] = np.zeros(n_features)\n",
        "            self.biases[cls] = 0\n",
        "\n",
        "            # Create binary labels for the current class (1 for cls, -1 for others)\n",
        "            y_binary = np.where(y == cls, 1, -1)\n",
        "\n",
        "            # Training loop\n",
        "            for _ in range(self.n_iterations):\n",
        "                correct_predictions = 0\n",
        "                for idx in range(n_samples):\n",
        "                    # Predict using the current weights and bias\n",
        "                    y_pred = np.sign(np.dot(self.weights[cls], X[idx]) + self.biases[cls])\n",
        "                    if y_pred == y_binary[idx]:\n",
        "                        correct_predictions += 1\n",
        "                    else:\n",
        "                        # Update weights and bias if prediction is incorrect\n",
        "                        update = self.alpha * y_binary[idx] * sample_weights[idx]\n",
        "                        self.weights[cls] += update * X[idx]\n",
        "                        self.biases[cls] += update\n",
        "\n",
        "                # Stop training early if weak threshold is met\n",
        "                accuracy = correct_predictions / n_samples\n",
        "                if accuracy > weak_threshold:\n",
        "                    break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for input data.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input data, shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted class labels, shape (n_samples,).\n",
        "        \"\"\"\n",
        "        scores = {cls: np.dot(X, self.weights[cls]) + self.biases[cls] for cls in self.classes}\n",
        "        return np.array([self.classes[np.argmax([scores[cls][i] for cls in self.classes])] for i in range(X.shape[0])])\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute the accuracy of the perceptron.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input data.\n",
        "            y (np.ndarray): True labels.\n",
        "\n",
        "        Returns:\n",
        "            float: Accuracy score.\n",
        "        \"\"\"\n",
        "        return np.mean(self.predict(X) == y)"
      ],
      "metadata": {
        "id": "svhf1P9sTwB1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-class Adaboost Algorithm :"
      ],
      "metadata": {
        "id": "XiMpfPpeUdTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiClassAdaboost:\n",
        "    \"\"\"\n",
        "    Implementation of the Multi-Class Adaboost algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weak_learner=MultiClassPerceptron, n_estimators=50):\n",
        "        \"\"\"\n",
        "        Initialize the Adaboost ensemble with weak learners.\n",
        "\n",
        "        Args:\n",
        "            weak_learner (class): Weak learner class (default: MultiClassPerceptron).\n",
        "            n_estimators (int): Number of weak learners to train.\n",
        "        \"\"\"\n",
        "        self.n_estimators = n_estimators\n",
        "        self.weak_learner = weak_learner\n",
        "        self.weak_learners = []\n",
        "        self.alphas = []\n",
        "        self.classes = None\n",
        "\n",
        "    def train(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the Adaboost ensemble.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Training data, shape (n_samples, n_features).\n",
        "            y (np.ndarray): Training labels, shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "            self: Trained MultiClassAdaboost instance.\n",
        "        \"\"\"\n",
        "        self.classes = np.unique(y)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize sample weights\n",
        "        sample_weights = np.ones(n_samples) / n_samples\n",
        "\n",
        "        for t in range(self.n_estimators):\n",
        "            # Train a weak learner\n",
        "            weak_learner = self.weak_learner().train(X, y, sample_weights)\n",
        "            self.weak_learners.append(weak_learner)\n",
        "\n",
        "            # Get predictions and calculate weighted error\n",
        "            weak_predictions = weak_learner.predict(X)\n",
        "            incorrect = (weak_predictions != y)\n",
        "            weighted_error = np.sum(sample_weights * incorrect)\n",
        "\n",
        "            # Stop if the weak learner is no better than random guessing\n",
        "            if weighted_error >= 0.5:\n",
        "                break\n",
        "\n",
        "            # Calculate alpha (importance of the weak learner)\n",
        "            alpha = 0.5 * np.log((1 - weighted_error) / (weighted_error + 1e-10))\n",
        "            self.alphas.append(alpha)\n",
        "\n",
        "            # Update sample weights\n",
        "            sample_weights *= np.exp(alpha * incorrect)\n",
        "            sample_weights /= np.sum(sample_weights)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels using the ensemble of weak learners.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input data, shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted class labels, shape (n_samples,).\n",
        "        \"\"\"\n",
        "        votes = np.zeros((X.shape[0], len(self.classes)))\n",
        "\n",
        "        for learner, alpha in zip(self.weak_learners, self.alphas):\n",
        "            predictions = learner.predict(X)\n",
        "            for i, pred in enumerate(predictions):\n",
        "                class_idx = np.where(self.classes == pred)[0][0]\n",
        "                votes[i, class_idx] += alpha\n",
        "\n",
        "        return self.classes[np.argmax(votes, axis=1)]\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute the accuracy of the Adaboost model.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input data.\n",
        "            y (np.ndarray): True labels.\n",
        "\n",
        "        Returns:\n",
        "            float: Accuracy score.\n",
        "        \"\"\"\n",
        "        return np.mean(self.predict(X) == y)\n"
      ],
      "metadata": {
        "id": "dWiPC5YhT7-b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main script for data loading, preprocessing, training, and evaluation"
      ],
      "metadata": {
        "id": "pB2R25NPT-37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Multi-Class Adaboost\n",
        "adaboost = MultiClassAdaboost(n_estimators=50)\n",
        "adaboost.train(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = adaboost.predict(X_test_scaled)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy9jOq42T-l4",
        "outputId": "b4d8edbb-4b5e-4e72-fe93-18d737271529"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9278\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99        36\n",
            "           1       0.90      0.75      0.82        36\n",
            "           2       0.94      0.97      0.96        35\n",
            "           3       0.97      0.92      0.94        37\n",
            "           4       0.97      0.97      0.97        36\n",
            "           5       0.97      0.95      0.96        37\n",
            "           6       1.00      0.92      0.96        36\n",
            "           7       0.97      1.00      0.99        36\n",
            "           8       0.81      0.86      0.83        35\n",
            "           9       0.79      0.94      0.86        36\n",
            "\n",
            "    accuracy                           0.93       360\n",
            "   macro avg       0.93      0.93      0.93       360\n",
            "weighted avg       0.93      0.93      0.93       360\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[36  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 27  0  1  1  0  0  0  3  4]\n",
            " [ 0  1 34  0  0  0  0  0  0  0]\n",
            " [ 0  0  2 34  0  0  0  0  0  1]\n",
            " [ 0  0  0  0 35  0  0  0  0  1]\n",
            " [ 0  0  0  0  0 35  0  0  0  2]\n",
            " [ 1  0  0  0  0  0 33  0  2  0]\n",
            " [ 0  0  0  0  0  0  0 36  0  0]\n",
            " [ 0  2  0  0  0  1  0  1 30  1]\n",
            " [ 0  0  0  0  0  0  0  0  2 34]]\n"
          ]
        }
      ]
    }
  ]
}
